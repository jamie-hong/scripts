{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Solution 6\n",
    "\n",
    "Ming Hong (mh4286@nyu.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 \n",
    "\n",
    "D. Bindel and J. Goodman: Principles of Scientific Computing, Chapter 6, Exercise 7.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "**(a)** In the steepest decsent method, we choose the gradient direction that leads to the steepest descent. Concretely, if we have a differentiable function\n",
    "$\\renewcommand{bs}{\\boldsymbol}$\n",
    "\n",
    "$$ V(\\bs x^k + t_k p_k ) \\approx V(\\bs x^k) + t_k [ {\\bs\\nabla} V(\\bs x^k)^T p_k ], $$\n",
    "\n",
    "then we choose $ p_k = - {\\bs\\nabla} V(\\bs x^k)$.\n",
    "\n",
    "Using exact linear search, we would choose $t_k$ that minimizes $\\phi(t_k) = V(\\bs x^{k} + t_k p_k)$ at each step $k$. Setting the first order condition to 0, we have\n",
    "\n",
    "$$ \\phi'(t_k) = 0 =  [{\\bs\\nabla} V(\\bs x^k + t_k p_k)]^T p_k, $$\n",
    "\n",
    "where ${\\bs\\nabla} V(\\bs x^k + t_k p_k) = - p_{k+1}$.\n",
    "\n",
    "Hence, $p_k \\cdot p_{k+1} = 0$, meaning $p_k$ and $p_{k+1}$ must be orthogonal for any $V$ in order to have the optimal step size $t_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** If $p_k$ is in the direction of $(-1,-1)^T$, according to the conclusion from Part (a), $p_{k+1}$ must be in the direction of $(-1, 1)^T$ or $(1, -1)^T$, which depends on the relative values between $x_1$ and $x_2$. \n",
    "\n",
    "More concretely, since $\\bs\\nabla V (\\bs x) = (\\lambda_1 x_1, \\lambda_2 x_2)^T$, we have the following at step $k$:\n",
    "\n",
    "$$ \\phi'(t) = 0 = \\left( \\begin{array} \\\\ \\lambda_1 \\cdot (x_1 - t \\lambda_1 x_1) \\\\ \\lambda_2 \\cdot(x_2 - t \\lambda_2 x_2) \\end{array} \\right)^T \\cdot \\left( \\begin{array} \\\\-\\lambda_1 x_1 \\\\ -\\lambda_2 x_2 \\end{array} \\right) $$\n",
    "\n",
    "Solving for $t$ yields \n",
    "\n",
    "$$ t = \\frac{2} {\\lambda_1 + \\lambda_2}$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$ p_{k+1} = \\left( \\begin{array} \\\\ -\\lambda_1 \\cdot (x_1 - t \\lambda_1 x_1) \\\\ -\\lambda_2 \\cdot(x_2 - t \\lambda_2 x_2) \\end{array} \\right) = \\left( \\begin{array} \\\\ \\lambda_1 x_1 \\frac{\\lambda_1 - \\lambda_2}{\\lambda_1 + \\lambda_2} \\\\ \\lambda_2 x_2 \\frac{\\lambda_2 - \\lambda_1}{\\lambda_1 + \\lambda_2} \\end{array} \\right) $$\n",
    "\n",
    "Note that $\\lambda_1 x_1 = \\lambda_2 x_2$ at step $k$, as $p_k$ is in the direction of $(-1,-1)^T$. Hence, $p_{k+1}$ must be in the direction of $(-1,1)^T$ (if $\\lambda_1 < \\lambda_2)$ or $(1,-1)^T$ (if $\\lambda_1 > \\lambda_2)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** As illustrated earlier, \n",
    "\n",
    "$$ p = - \\bs\\nabla V (\\bs x) = \\left( \\begin{array} \\\\ - \\lambda_1 x_1  \\\\ - \\lambda_2 x_2 \\end{array} \\right), $$\n",
    "\n",
    "which is in the direction of $(-1,-1)^T$ if and only if $\\lambda_1 x_1 = \\lambda_2 x_2$. Alternatively, $(x_1,x_2) = r(\\lambda_2,\\lambda_1)$ for some $r$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** If $\\bs x = \\left( \\begin{array} \\\\ x_1 \\\\ x_2 \\end{array} \\right) $ at step $k$, then at step $k+1$ we have\n",
    "\n",
    "$$ \\bs x = \\left( \\begin{array} \\\\ x_1 - t \\lambda_1 x_1 \\\\ x_2 - t \\lambda_2 x_2 \\end{array} \\right) \n",
    "         = \\left( \\begin{array} \\\\ x_1 \\frac{\\lambda_2-\\lambda_1}{\\lambda_1+\\lambda_2}  \n",
    "                  \\\\ x_2 \\frac{\\lambda_1-\\lambda_2}{\\lambda_1+\\lambda_2} \\end{array} \\right)\n",
    "         = \\frac{\\lambda_1-\\lambda_2}{\\lambda_1+\\lambda_2} \\left( \\begin{matrix}  -x_1 \\\\ x_2 \\end{matrix} \\right)$$\n",
    "\n",
    "Since the opitmium is $x_* = (0,0)^T$, the error is simply $\\Vert \\bs x \\Vert$. If $p_0$ is in the direction of $(-1,-1)^T$ and assuming $\\lambda_1 \\ge \\lambda_2$, then we have the ratio of the errors as\n",
    "\n",
    "$$ \\rho = \\frac{\\lambda_1 - \\lambda_2}{\\lambda_1 + \\lambda_2}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** If $\\lambda_1 \\gg \\lambda_2$, we have \n",
    "\n",
    "$$ \\rho = \\frac{\\lambda_1 - \\lambda_2}{\\lambda_1 + \\lambda_2} = \\frac{\\lambda_1 + \\lambda_2 - 2\\lambda_2}{\\lambda_1 + \\lambda_2} = 1 - \\frac{2\\lambda_2}{\\lambda_1 + \\lambda_2} \\approx 1 - 2\\frac{\\lambda_2}{\\lambda_1} $$\n",
    "\n",
    "Since $ V(\\bs x) = \\frac{1}{2} (\\lambda_1 x_1^2 + \\lambda_2 x_2^2) $, the Hessian matrix is $ H = \\left( \\begin{matrix} \\lambda_1 & 0 \\\\ 0 & \\lambda_2 \\end{matrix} \\right) $. Thus, the condition number of $H$ is\n",
    "\n",
    "$$ \\kappa(H) = \\Vert H \\Vert \\Vert H^{-1} \\Vert = \\left\\Vert \\left( \\begin{matrix} \\lambda_1 & 0 \\\\  0 & \\lambda_2 \\end{matrix} \\right) \\right\\Vert \\cdot \\left\\Vert \\left( \\begin{matrix} \\frac{1}{\\lambda_1} & 0 \\\\ 0 & \\frac{1}{\\lambda_2} \\end{matrix} \\right) \\right\\Vert =  \\lambda_1 \\cdot \\frac{1}{\\lambda_2} $$\n",
    "\n",
    "Thus, we have\n",
    "\n",
    "$$ \\rho \\approx 1 - 2\\frac{\\lambda_2}{\\lambda_1} = 1 - \\frac{2}{\\kappa(H)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f)** It is easy to see that in order to decrease the error by a factor of $e^{2}$ after $n$ iterations, we have\n",
    "\n",
    "$$ e^{-2} \\approx \\rho^n \\approx (1 - \\frac{2}{\\kappa(H)})^n, $$\n",
    "\n",
    "where $n \\approx \\kappa(H)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "For the quadratic programming problem derived for portfolio optimization\n",
    "\n",
    "$$\n",
    "\\begin{array}\n",
    "\\\\\n",
    "\\min_{\\bs x} & &  \\frac{1}{2} \\lambda\\; {\\bs x}^T \\Sigma {\\bs x} - \\bs \\mu^T {\\bs x} \n",
    "\\\\\n",
    "s.t. & & \\bs\\iota^T \\bs x = 1 \\end{array} $$\n",
    "\n",
    "where $\\lambda$ is the risk-aversion coefficient, $\\bs \\mu$ is the expected asset return vector, $\\Sigma$ is the covariance matrix, and $\\bs\\iota$ is a vector of one's.  Derive the dual problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "The primal problem is:\n",
    "\n",
    "$$ \\begin{array} \\\\ \\min_{\\bf x}  &  f(\\bs x) \\\\ s.t. & h(\\bs x) = 0 \\end{array} $$\n",
    "\n",
    "where $ f(\\bs x) = \\frac{1}{2} \\lambda\\; {\\bs x}^T \\Sigma {\\bs x} - \\bs\\mu^T {\\bs x} $, and $ h(\\bs x) = \\bs\\iota^T \\bs x -1 $.\n",
    "\n",
    "The Lagrangian of this optimization problem is\n",
    "\n",
    "$$ L(\\bs x, \\gamma) = f(\\bs x) + \\gamma h(\\bs x) $$\n",
    "\n",
    "The Lagrangian dual function is hence\n",
    "\n",
    "$$ \\begin{align} \\hat{f}(\\gamma) & = \\inf_{\\bs x \\in D} L(\\bs x, \\gamma) = \\inf_{\\bs x \\in D} \\left( f(\\bs x) + \\gamma                                       h(\\bs x) \\right) \\\\\n",
    "                                 & = \\inf_{\\bs x \\in D} \\left( \\frac{1}{2} \\lambda\\; {\\bs x}^T \\Sigma {\\bs x} - \\bs\\mu^T {\\bs x} + \\gamma \\left( \\bs\\iota^T \\bs x-1 \\right) \\right) \\end{align} $$\n",
    "\n",
    "If $\\bs x^*$ is a solution to the original primal problem, then for any $\\gamma$, \n",
    "\n",
    "$$ \\hat{f}(\\gamma) \\le f(\\bs x^*) $$\n",
    "\n",
    "Hence, the dual problem can be formulated as\n",
    "\n",
    "$$ \\max_{\\gamma}  \\;\\;  \\hat{f}(\\gamma) = \\inf_{\\bs x \\in D} \\left( \\frac{1}{2} \\lambda\\; {\\bs x}^T \\Sigma {\\bs x} - \\bs\\mu^T {\\bs x} + \\gamma \\left( \\bs\\iota^T \\bs x-1 \\right) \\right) $$\n",
    "\n",
    "The optimality conditions are\n",
    "\n",
    "1. $ \\bs{\\nabla}_{\\bs x} L(\\bs x^*, \\gamma^*) = \\bs{0} $: stationality \n",
    "2. $ h(\\bs x^*)  = 0$: feasibility  \n",
    "3. $ \\bs{\\nabla}^2 L(\\bs x^*, \\gamma^*) \\succ 0 $:  Hessian positive definite constraints\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
